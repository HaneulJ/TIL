## 4/12 수업

#### 사이킷런에서 지원하는 원핫인코딩 구현

* LabelEncoder : 문자형태의 데이터로 구성된 피쳐를 수치화 함

* OneHotEncoder : 숫자로 표현된 범주형 데이터를 인코딩 필요

* 희소행렬(Sparse Matrix)로 만들어지므로 array로 만들려면 `.toarray()` 필요

* LabelBinarizer 클래스를 사용해서 (텍스트 범주에서 숫자 범주로 바꾸고, 숫자 범주에서 원-핫 벡터로 바꾸는) 두개의 변환을 한번에 적용할 수 있음



### 머신러닝 데이터 분석

#### 머신러닝이란

> 스스로 데이터를 학습하여 서로 다른 변수 간의 관계를 찾아 나가는 과정

* 예측, 분류, 군집 알고리즘

#### 지도 학습 vs 비지도 학습

* 지도 학습: 정답 데이터를 다른 데이터와 함께 컴퓨터 알고리즘에 입력하는 방식
  * 회귀분석
  * 분류
* 비지도 학습: 정답 데이터 없이 컴퓨터 알고리즘 스스로 데이터로부터 숨은 패턴을 찾아내는 방식
  * 정답이 없는 상태에서 서로 비슷한 데이터를 찾아서 그룹화
  * 군집 분석
  * 차원 축소



#### 머신러닝 프로세스

* 컴퓨터 알고리즘이 이해할 수 있는 형태로 데이터 변환 작업 선행 
* 분석 대상에 관해 수집한 관측값을 속성을 기준으로 정리
* 훈련 데이터 / 검증 데이터
* 데이터 정리 > 데이터 분리(훈련/검증) > 알고리즘 준비 > 모형 학습(훈련 데이터) > 예측(검증 데이터) > 모형 평가 > 모형 활용





#### 회귀분석

> 머신러닝 알고리즘, 연속 변속를 예측

* 가격, 매출, 주가 ,환율, 수량 
* 종속 변수(예측 변수): 분석 모형이 예측하고자 하는 목표
* 독립 변수(설명 변수): 예측을 위해 모형이 사용하는 속성
* 결정계수는 추정한 선형 모형이 주어진 자료에 적합한 정도를 재는 척도 임
  * 결정계수의 값은 0에서 1사이에 있으며, 종속변인과 독립변인 사이에 상관관계가 높을수록 1에 가까워짐 
  * 결정계수가 0에 가까운 값을 가지는 회귀모형은 유용성이 낮은 반면, 결정계수의 값이 클수록 회귀모형의 유용성이 높다고 판단함



##### 단순회귀분석

> 두 변수 사이에 일대일로 대응되는 확률적, 통계적 상관성을 찾는 알고리즘

* Y = aX + b
* 변수 X와 Y에 대한 정보로, 일차 방정식의 계수 a, b를 찾는 과정

* 데이터 탐색 `.info()`, `.describe()`, `.astype()`로 실수형으로 변환
* 속성 선택
* 훈련 / 검증 데이터 분할
* 모형 학습 및 검증: `fit`, 결정계수(R제곱) 값이 클수록 모형의 예측 능력이 좋다고 판단



#### 비선형 회귀분석

> 

##### 다항회귀분석

> 이차함수 이상의 다항 함수를 이용하여 두 변수 간의 선형관계를 설명하는 알고리즘




##### 다중회귀분석

> 여러 개의 독립 변수가 종속 변수에 영향을 주고 선형 관계를 갖는 경우

* Y = b + a1X1 + a2X2 + .. + anXn





#### 분류

> 예측하려는 대상의 속성(설명 변수)을 입력 받고, 목표 변수가 갖고 있는 카테고리(범주형)값 중에서 어느 한 값으로 분류하여 예측

* 고객 분류, 질병 진단, 스팸 메일 필터링, 음석 인식 등
* KNN, SVM, Decision Tree, Logistic Regression



##### 성능평가지표

* confusion matrix(오차 행렬): TP, FP, TN, FN
  * 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표
  * Precision: 정밀도 = TP/(TP+FP)
  * Recall: 재현율 = TP/(TP+FN)
  * F1: 정확도와 재현율의 조화평균 = 2 (P * R) / P + R
    * 정밀도와 재현율을 결합한 지표로, 정밀도와 재현율이 어느 한 쪽으로 치우치지 않을 때 상대적으로 높은 값을 가진다. 
  * 정밀도와 재현율은 서로 보완적인 분류 성능 지표이며 가장 좋은 모델은 재현율과 정밀도의 수치가 둘다 높은 것
* ROC 곡선과 AUC
  * 의학 분야나 이진 분류 모델의 성능 평가 지표로 중요하게 사용
  * ROC 곡선: FPR의 변화에 따른 TRP의 변화를 나타내는 곡선
  * FPR (False Positive Rate): FPR = FP / (FP+TN) 으로, 실제 Negative 를 잘못 예측한 비율
  * TPR (True Positive Rate): TPR = TP / (FN+TP)으로, 재현율과 같이 실제 Positive인 것 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율
* 정확도(Accuracy) = (예측 결과가 동일한 데이터 건수) / (전체 예측 데이터 건수)
  * 실제 데이터에 예측 데이터가 얼마나 같은지를 판단하는 지표
  * (TP + TN) / (TP + TN + FP + FN)



##### KNN

> K-Nearest-Neighbors

* 새로운 관측값이 주어지면 기존 데이터 중에서 가장 속성이 비슷한 k개의 이웃을 먼저 찾는다
* 가까운 이웃들이 갖고 있는 목표 값과 같은 값으로 분류하여 예측
* k = 이웃의 개수



##### SVM

>  Support Vector Machine

* 벡터 공간에 위치한 훈련 데이터의 좌표와 각 데이터가 어떤 분류 값을 가져야 하는지 정답을 입력받아서 학습



##### Decision Tree

> 의사결정 나무

* 트리 구조를 사용하고, 각 분기점(node)에는 분석 대상의 속성(설명 변수)들이 위치
* 각 분기점마다 목표 값을 가장 잘 분류할 수 있는 속성을 찾아서 배치, 해당 속성이 갖는 값을 이용하여 새로운 가지(branch)를 만든다.



#### 군집

> 데이터셋의 관측값이 갖고 있는 여러 속성을 분석하여 서로 비슷한 특징을 갖는 관측값끼리 같은 클러스터(집단)로 묶는 알고리즘

* 특이 데이터(이상값, 중복값 등) 찾는데 활용
* 비지도학습
* 정답이 없는 상태에서 데이터 자체의 유사성만을 기준으로 판단
* 신용카드 부정 사용 탐지, 구매 패턴 분석 등 소비자 행동 특성 그룹화
* 같은 집단 내의 다른 소비자를 통해 새로운 소비자의 구매 패턴이나 행동 등을 예측



#####  k-means

> 데이터 간의 유사성 측정

* k = 나눌 군집의 개수(기본 = 8)
* 일반적으로 k가 클수록 모형의 정확도는 개선되지만, k값이 너무 커지면 선택지가 너무 많아지므로 분석의 효과가 사라짐

* `standardScaler`



##### DBSCAN

> Density Based Spatial Clustering of Applications with Noise

* 데이터가 위치하고 있는 공간 밀집도를 기준으로 클러스터 구분

* 클러스터 수 지정X





#### Scikit-learn

> python 프로그래밍 언어를 위한 무료 소프트웨어 기계 학습 라이브러리

##### Estimate

* `fit()`
* `predict()`



#### PCA : 주성분분석

> 고차원의 데이터를 저차원의 데이터로 환원시키는 기법

* 데이터를 어떤 기준을 바탕으로 변환을 하고, 그 변환으로 인해 '주성분'이 추출

* 추출된 주성분은 원래 가지고 있는 데이터와 다르며 변환된 데이터
* 변수의 의미가 중요한 경우에는 PCA를 사용하면 안 된다. 
  * PCA는 데이터에 변환을 가하는 것이기 때문이다.

* PCA의 본질: 차원 축소
* 차원이 축소됐다는 것은 원본 데이터가 아니라 변환(projection)된 데이터, 즉 주성분을 이용해 분석 혹은 모델링을 진행



#### 연관규칙 ,연관성분석 (association analaysis) - 비지도 학습

>  대량의 데이터에 숨겨진 항목간의 연관규칙을 찾아내는 기법 

* 장바구니 분석(market basket analysis)
* 실제 연관성 분석은 월마트, 아마존 등 여러기업에서 다양한 마케팅 활동에 활용하고 있으며 더 나아가 사회 네트워크 분석에도 활용할 수 있다.



##### 빈발패턴

>  주어진 데이터 셋에서 빈발하게 발생하는 패턴을 찾아내는 기법



##### 연관규칙

> 빈발패턴들 간의 연관성을 규칙으로 찾아내는 기법

* 장점
  * 대규모 거래 데이터에 대해 작업을 할 수 있다.
  * 이해하기 쉬운 규칙을 생성해준다.
  * 데이터마이닝과 데이터 베이스에서 예상치 못한 지식을 발굴하는데 유용하다.

* 단점
  * 작은 데이터셋에는 그다지 유용하지 않다
  * 진정한 통찰력과 상식을 분리하기 위한 노력이 필요하다.



##### 지지도(support)

>  전체 거래중 연관성 규칙을 구성하는 항목들이 포함된 거래의 비율

* support = 항목에 대한 거래수 / 전체 거래수  또는  A,B가 동시에 포함된 거래수 / 전체 거래수
* 규칙의 중요성 - 필요조건



##### 신뢰도(confidence) 

> 연관성의 정도

* 규칙의 신뢰성 - 충분조건

* 항목 A를 포함하는 거래 중에서 항목 A와 항목 B가 같이 포함될 확률

* confidence = 조건과 결과 항목을 동시에 포함하는 거래수 / 조건항목을 포함한 거래수  또는 A,B가 동시에 포함된 거래수 / A를 포함하는 거래수
                            

##### 향상도(lift)

- 지지도와 신뢰도를 동시에 고려한다.
- 향상도 값이 1인 경우 조건과 결과는 우연에 의한 관계라고 보며 1보다 클수록 우연이 아닌 의미있는 연관성을 가진 규칙이라고 해석한다.
- **'높은 상관 관계를 가진'** 것들 만으로 추려내기 위한 지표
- 어떤 연관 규칙이 정말 연관성이 있는지 판단하려면 향상도가 1보다 큰 수치로 나타나야 함.
- 향상도 = A,B동시구매 비율 / A구매 비율*B구매 비율



* 빈발항목중에서 후보집합을 선정하는 기준으로 지지도(Support) 를 사용



#### 트위터 API로 연관 키워드 분석하기

* API 데이터로 데이터 프레임 생성하기

- 텍스트 데이터 전처리
- nltk, konlpy를 이용한 키워드 추출
- 연관 키워드 추출하기
- 단어 빈도 추출하기
- 연관 키워드 네트워크 시각화