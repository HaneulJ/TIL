## 4/8 수업

'탐색적 데이터 분석(EDA)’은 우리가 존재한다고 믿는 것들은 물론이고 존재하지 않는다고 믿는 것들을 발견하려는 태도, 유연성, 그리고 자발성이다.  - 존 튜키

1. **데이터에 대한 질문 & 문제 만들기**

2. **데이터를 시각화하고, 변환하고, 모델링하여 그 질문 & 문제에 대한 답을 찾아보기**

3. **찾는 과정에서 배운 것들을 토대로 다시 질문을 다듬고 또 다른 질문 & 문제 만들기**



* 전체적으로 데이터의 속성들을 살펴본다 - 결측치와 이상치 체크와 대처

* 속성 간의 관계 분석 - 타입, 범주형 연속형 체크, 시각화와 상관계수를 통한 상관성 체크



#### EDA의 핵심 - 데이터의 결측치와 특이값(이상치) 채크와 처리

* 데이터 읽어서 정보 보기



#### EDA 작업이후 데이터 전처리에서 필요한 작업 - 범주형 데이터 전처리

* 사이킷런은 문자열 값을 입력 값으로 처리 하지 않기 때문에 숫자 형으로 변환해야 함

* 범주형 변수의 경우 전처리를 통해 정수값으로 변환
* 범주형이 아닌 단순 문자열인 경우 일반적으로 제거
* 범주형 변수(피쳐)의 처리 방법
  * 레이블 인코딩
  * 더미화(원 핫 인코딩)



* **레이블 인코딩(Label encoding)**

  * 문자열(범주형) 값을 0 부터 1씩 증가하는 값으로 변환
  * 숫자의 차이가 모델에 영향을 주지 않는 트리 계열 모델(의사결정나무, 랜덤포레스트)에 적용

  * 숫자의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에는 사용하지 않음
  * **encoding_columns 레이블 인코딩 처리**
  * 범주형: `workclass`, `education`, `education-num`, `marital-status`, `occupation`, `relationship`, `race`, `gender`, `hours-per-week`, `native-country`, `income`
  * 연속형: `age`, `fnlwgt`, `capital-gain`, `capital-loss`
  * **`sklearn.preprocessing.LabelEncode`r 사용**
  * `fit()` : 어떻게 변환할 지 학습
  * `transform()`: 문자열를 숫자로 변환
  * `fit_transform()` : 학습과 변환을 한번에 처리 (fit + transform)
  * `inverse_transform()` : 숫자를 문자열로 변환
  * `classes_ `: 인코딩한 클래스 조회



* **더미화 - 원핫 인코딩(One-Hot encoding)**

  * N개의 클래스를 N 차원의 One-Hot 벡터로 표현되도록 변환
  * 고유값들을 피처로 만들고 정답에 해당하는 열은 1로 나머진 0으로 표시
  * 변환해야 하는 값의 종류가 여러 개일 때
  * 숫자의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에서 범주형 데이터 변환시 라벨 인코딩 보다 원핫 인코딩을 사용
  * **`pandas.get_dummies(DataFrame [, columns=[변환할 컬럼명]])`** 함수 이용, DataFrame에서 범주형(문자열) 변수만 변환

  * **`sklearn.preprocessing.OneHotEncoder`**이용, DataFrame을 넣을 경우 모든 변수들을 변환하므로 범주형 컬럼만 처리하도록 해야 함
  * `fit()`: 어떻게 변환할 지 학습
  * `transform()`: 문자열를 숫자로 변환
  * `fit_transform()`: 학습과 변환을 한번에 처리
  * `get_feature_names()` : 원핫인코딩으로 변환된 컬럼의 이름을 반환

  



#### EDA 작업이후 데이터 전처리에서 필요한 작업 - 수치형 데이터 전처리

>  각 열(변수, 피처, 속성)이 가지는 값들의 숫자 범위(Scale)가 다를 경우 이 값의 범위를 일정한 범위로 맞추는 작업

**Normalization(정규화) / Standardization(표준화)**

* 데이터 분석을 수행하면서 많이 겪는 문제중 하나가 **데이터 단위의 불일치**
* 칼럼간에 데이터의 단위가 다르면 칼럼마다 스케일이 크게 차이가 나게 되고 분석 결과에 영향을 주게 된다.
* 이를 해결하는 방법으로 **Normalization(정규화)**과 **Standardization(표준화)**가 있음



 ##### [ 정규화(normalization)]

* 데이터의 상대적 크기에 대한 영향을 줄이기 위해 데이터범위를 0~1로 변환

* MinMax스케일러라고도 함
* 2개 이상의 대상 컬럼(변수, 피쳐, 속성)의 단위가 다를 때 대상 데이터를 같은 기준으로 볼 수 있게 함
* 식 : (측정값 - 최소값) / (최대값 - 최소값)



##### [표준화(Standardization)]  Z-score 표준화

* 피쳐의 값들이 평균이 0이고 표준편차가 1인 범위(표준정규분포)에 있도록 변환
* 데이터가 평균으로부터 얼마나 떨어져있는지 나타내는 값으로, 특정 범위를 벗어난 데이터는 outlier로 간주, 제거 
* 특히 SVM이나 선형회귀, 로지스틱 회귀 알고리즘(선형모델)은 데이터셋이 표준정규분포를 따를때 성능이 좋은 모델이기 때문에 표준화를 하면 대부분의 경우 성능이 향상
* 데이터를 0을 중심으로 양쪽으로 데이터를 분포시키는 방법
* 표준화를 하면 각 데이터들이 평균을 기준으로 얼마나 떨여져 있는지를 나타내는 값으로 변환
* 식(Z-score 표준화) : **(측정값 - 평균) / 표준편차**



* 트리계열을 제외한 대부분의 머신러닝 알고리즘들이 피처의 스케일에 영향을 받음
* 선형모델, SVM 모델, 신경망 모델 등
* 특히 SVM이나 선형회귀, 로지스틱 회귀 알고리즘(선형모델)은 데이터셋이 표준정규분포를 따를때 성능이 좋은 모델이기 때문에 표준화를 하면 대부분의 경우 성능이 향상됨

* 데이터 표준화를 통해 outlier를 제거한 다음 데이터 정규화를 하여 상대적크기에 대한 영향력을 줄인 다음 분석을 시작한다.